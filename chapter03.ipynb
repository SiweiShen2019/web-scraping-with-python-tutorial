{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 Writing Web Crawlers\n",
    "## Traversing a Single Domain\n",
    "\n",
    "To review what we have learned in previous chapters, we're going to write Python snippets to retrieve a list of links on any arbitrary Wikipedia page. Although Wikipedia has its own API to help us get access to data more efficiently, we practice on Wikipedia because 1) it's stable; 2) it has a simple HTML structure. We use Kevin Bacon's page as an example: [http://en.wikipedia.org/wiki/Kevin_Bacon](http://en.wikipedia.org/wiki/Kevin_Bacon)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "856\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "wiki_url = \"http://en.wikipedia.org/wiki/Kevin_Bacon\"\n",
    "\n",
    "http_res = urlopen(wiki_url)\n",
    "bs = BeautifulSoup(http_res, 'html.parser')\n",
    "links = []\n",
    "for link in bs.find_all('a'):\n",
    "    if \"href\" in link.attrs:\n",
    "        links.append(link.attrs[\"href\"])\n",
    "print(len(links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are too many entries in the list, we print out the first 15 for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/wiki/Wikipedia:Protection_policy#semi\n",
      "#mw-head\n",
      "#p-search\n",
      "/wiki/Kevin_Bacon_(disambiguation)\n",
      "/wiki/File:Kevin_Bacon_SDCC_2014.jpg\n",
      "/wiki/Philadelphia\n",
      "/wiki/Pennsylvania\n",
      "/wiki/Kyra_Sedgwick\n",
      "/wiki/Sosie_Bacon\n",
      "/wiki/Edmund_Bacon_(architect)\n",
      "/wiki/Michael_Bacon_(musician)\n",
      "http://baconbros.com/\n",
      "#cite_note-1\n",
      "#cite_note-actor-2\n",
      "/wiki/Footloose_(1984_film)\n"
     ]
    }
   ],
   "source": [
    "for link in links[:15]:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results, we find that there're still non-Wiki links. For example, \"#mw-head\" and \"http://baconbros.com\". But Wiki links have the same pattern \"*/wiki/*\". Therefore, we use Regex `^(/wiki/)((?!:).)*$` to help us to filter out Wiki links. Unlike the textbook, we use Regex on `links` rather than in `find_all()` here because we don't want to parse the data again, which takes a long time if we have a huge amount of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "wiki_links = []\n",
    "for link in links:\n",
    "    if re.match(r\"^(/wiki/)((?!:).)*$\", link):\n",
    "        wiki_links.append(link)\n",
    "print(len(wiki_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/wiki/Kevin_Bacon_(disambiguation)\n",
      "/wiki/Philadelphia\n",
      "/wiki/Pennsylvania\n",
      "/wiki/Kyra_Sedgwick\n",
      "/wiki/Sosie_Bacon\n",
      "/wiki/Edmund_Bacon_(architect)\n",
      "/wiki/Michael_Bacon_(musician)\n",
      "/wiki/Footloose_(1984_film)\n",
      "/wiki/JFK_(film)\n",
      "/wiki/A_Few_Good_Men\n",
      "/wiki/Apollo_13_(film)\n",
      "/wiki/Mystic_River_(film)\n",
      "/wiki/Sleepers\n",
      "/wiki/The_Woodsman_(2004_film)\n",
      "/wiki/Fox_Broadcasting_Company\n"
     ]
    }
   ],
   "source": [
    "for link in wiki_links[:15]:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawling an Entire Site\n",
    "\n",
    "Now that you have had a taste of web scraping, you are able to crawl an entire Wikipedia! To do so, we also need a deeper understanding of the structure of Wikipedia:\n",
    "\n",
    "- All titles are under `h1->span` tags\n",
    "- First paragraph of text lives under `div#mw-content-text->p`.\n",
    "- Edit links are under `span#wb-langlinks-edit->a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "pages = set()\n",
    "\n",
    "def get_links(pageUrl):\n",
    "    global pages\n",
    "    http_res = urlopen(\"http://en.wikipedia.org{}\".format(pageUrl))\n",
    "    bs = BeautifulSoup(http_res, \"html.parser\")\n",
    "    try:\n",
    "        print(bs.h1.get_text())\n",
    "        # uncomment to print first paragraph\n",
    "        # print(bs.find(id=\"mw-content-text\").find_all(\"p\")[0])\n",
    "        print(bs.find(\"span\", {\"class\": \"wb-langlinks-edit\"})\n",
    "                .find(\"a\").attrs[\"href\"])\n",
    "    except AttributeError:\n",
    "        print(\"This page is missing something! Continuing.\")\n",
    "    \n",
    "    # DFS\n",
    "    for link in bs.find_all(\"a\", href=re.compile(r\"^(/wiki/)\")):\n",
    "        if len(pages) >= 5:\n",
    "            break\n",
    "        if \"href\" in link.attrs:\n",
    "            if link.attrs[\"href\"] not in pages:\n",
    "                new_page = link.attrs[\"href\"]\n",
    "                print(\"-\"*20)\n",
    "                print(new_page)\n",
    "                pages.add(new_page)\n",
    "                get_links(new_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Page\n",
      "This page is missing something! Continuing.\n",
      "--------------------\n",
      "/wiki/Wikipedia\n",
      "Wikipedia\n",
      "https://www.wikidata.org/wiki/Special:EntityPage/Q52#sitelinks-wikipedia\n",
      "--------------------\n",
      "/wiki/Wikipedia:Protection_policy#semi\n",
      "Wikipedia:Protection policy\n",
      "https://www.wikidata.org/wiki/Special:EntityPage/Q4616470#sitelinks-wikipedia\n",
      "--------------------\n",
      "/wiki/Wikipedia:Requests_for_page_protection\n",
      "Wikipedia:Requests for page protection\n",
      "https://www.wikidata.org/wiki/Special:EntityPage/Q5478470#sitelinks-wikipedia\n",
      "--------------------\n",
      "/wiki/Wikipedia:Requests_for_permissions\n",
      "Wikipedia:Requests for permissions\n",
      "https://www.wikidata.org/wiki/Special:EntityPage/Q5453037#sitelinks-wikipedia\n",
      "--------------------\n",
      "/wiki/Wikipedia:Protection_policy#template\n",
      "Wikipedia:Protection policy\n",
      "https://www.wikidata.org/wiki/Special:EntityPage/Q4616470#sitelinks-wikipedia\n"
     ]
    }
   ],
   "source": [
    "get_links(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawling Across the Internet\n",
    "\n",
    "Here we give another example starting at [http://oreilly.com](http://oreilly.com) and randomly hopping from external link to external link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random external link is: https://www.facebook.com/OReilly/\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "pages = set()\n",
    "random.seed(datetime.datetime.now())\n",
    "\n",
    "#Retrieves a list of all Internal links found on a page\n",
    "def getInternalLinks(bs, includeUrl):\n",
    "    includeUrl = '{}://{}'.format(urlparse(includeUrl).scheme, urlparse(includeUrl).netloc)\n",
    "    internalLinks = []\n",
    "    #Finds all links that begin with a \"/\"\n",
    "    for link in bs.find_all('a', href=re.compile('^(/|.*'+includeUrl+')')):\n",
    "        if link.attrs['href'] is not None:\n",
    "            if link.attrs['href'] not in internalLinks:\n",
    "                if(link.attrs['href'].startswith('/')):\n",
    "                    internalLinks.append(includeUrl+link.attrs['href'])\n",
    "                else:\n",
    "                    internalLinks.append(link.attrs['href'])\n",
    "    return internalLinks\n",
    "            \n",
    "#Retrieves a list of all external links found on a page\n",
    "def getExternalLinks(bs, excludeUrl):\n",
    "    externalLinks = []\n",
    "    #Finds all links that start with \"http\" that do\n",
    "    #not contain the current URL\n",
    "    for link in bs.find_all('a', href=re.compile('^(http|www)((?!'+excludeUrl+').)*$')):\n",
    "        if link.attrs['href'] is not None:\n",
    "            if link.attrs['href'] not in externalLinks:\n",
    "                externalLinks.append(link.attrs['href'])\n",
    "    return externalLinks\n",
    "\n",
    "def getRandomExternalLink(startingPage):\n",
    "    html = urlopen(startingPage)\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    externalLinks = getExternalLinks(bs, urlparse(startingPage).netloc)\n",
    "    if len(externalLinks) == 0:\n",
    "        print('No external links, looking around the site for one')\n",
    "        domain = '{}://{}'.format(urlparse(startingPage).scheme, urlparse(startingPage).netloc)\n",
    "        internalLinks = getInternalLinks(bs, domain)\n",
    "        return getRandomExternalLink(internalLinks[random.randint(0,\n",
    "                                    len(internalLinks)-1)])\n",
    "    else:\n",
    "        return externalLinks[random.randint(0, len(externalLinks)-1)]\n",
    "    \n",
    "def followExternalOnly(startingSite):\n",
    "    externalLink = getRandomExternalLink(startingSite)\n",
    "    print('Random external link is: {}'.format(externalLink))\n",
    "    # uncomment to get \"infinite\" external links\n",
    "    # followExternalOnly(externalLink)\n",
    "            \n",
    "followExternalOnly('http://oreilly.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we can also get all unique external links using `set()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.oreilly.com\n",
      "https://www.oreilly.com/sign-in.html\n",
      "https://www.oreilly.com/online-learning/try-now.html\n",
      "https://www.oreilly.com/online-learning/index.html\n",
      "https://www.oreilly.com/online-learning/individuals.html\n",
      "https://www.oreilly.com/online-learning/teams.html\n",
      "https://www.oreilly.com/online-learning/enterprise.html\n",
      "https://www.oreilly.com/online-learning/government.html\n",
      "https://www.oreilly.com/online-learning/academic.html\n",
      "https://www.oreilly.com/online-learning/features.html\n",
      "https://www.oreilly.com/online-learning/pricing.html\n",
      "https://www.oreilly.com/blended-courses.html\n",
      "https://www.oreilly.com/conferences/\n",
      "https://www.oreilly.com/ideas/\n",
      "https://www.oreilly.com/about/approach.html\n",
      "https://conferences.oreilly.com/oscon/oscon-or\n",
      "https://www.oreilly.com/whats-new.html\n",
      "https://conferences.oreilly.com/strata/strata-eu\n",
      "https://conferences.oreilly.com/velocity/vl-ca\n",
      "https://learning.oreilly.com/register/\n",
      "https://learning.oreilly.com/team-setup/\n",
      "https://conferences.oreilly.com/software-architecture/sa-ca\n",
      "https://ai.oreilly.com.cn/ai-cn\n",
      "https://conferences.oreilly.com/artificial-intelligence/ai-ca\n",
      "http://shop.oreilly.com/category/customer-service.do\n",
      "https://twitter.com/oreillymedia\n",
      "https://www.facebook.com/OReilly/\n",
      "https://www.linkedin.com/company/oreilly-media\n",
      "https://www.youtube.com/user/OreillyMedia\n",
      "https://www.oreilly.com/emails/newsletters/\n",
      "https://itunes.apple.com/us/app/safari-to-go/id881697395\n",
      "https://play.google.com/store/apps/details?id=com.safariflow.queue\n"
     ]
    }
   ],
   "source": [
    "# Collects a list of all external URLs found on the site\n",
    "allExtLinks = set()\n",
    "allIntLinks = set()\n",
    "\n",
    "\n",
    "def getAllExternalLinks(siteUrl):\n",
    "    html = urlopen(siteUrl)\n",
    "    domain = '{}://{}'.format(urlparse(siteUrl).scheme,\n",
    "                              urlparse(siteUrl).netloc)\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    internalLinks = getInternalLinks(bs, domain)\n",
    "    externalLinks = getExternalLinks(bs, domain)\n",
    "    \n",
    "    for link in externalLinks:\n",
    "        if link not in allExtLinks:\n",
    "            allExtLinks.add(link)\n",
    "            print(link)\n",
    "    for link in internalLinks:\n",
    "        if link not in allIntLinks:\n",
    "            allIntLinks.add(link)\n",
    "            # uncomment to get \"all\" external links\n",
    "            # getAllExternalLinks(link)\n",
    "\n",
    "allIntLinks.add('http://oreilly.com')\n",
    "getAllExternalLinks('http://oreilly.com')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
