{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1 Your First Web Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting\n",
    "Although web browsers do us a big favor to decode data to render fantastic interfaces, in order to learn web scraping, we need to start at the level of the network connection.\n",
    "\n",
    "Here's a very basic function using Python standard library `urllib` to grab html contents from our [example website](http://pythonscraping.com/pages/page1.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<html>\\n<head>\\n<title>A Useful Page</title>\\n</head>\\n<body>\\n<h1>An Interesting Title</h1>\\n<div>\\nLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\\n</div>\\n</body>\\n</html>\\n'\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "http_res = urlopen(\"http://pythonscraping.com/pages/page1.html\")\n",
    "print(http_res.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Introduction to BeautifulSoup\n",
    "Using `urllib`, we can already get the html contents rendered on website. However, our example is simple for us to manually handle all tags. What if we have a more complex one that contains titles, panels, tables, and images? The answer is we need a library to handle this cleaning process for us. Here we have `BeautifulSoup`!\n",
    "\n",
    "Let's use the same example above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head>\n",
      "<title>A Useful Page</title>\n",
      "</head>\n",
      "<body>\n",
      "<h1>An Interesting Title</h1>\n",
      "<div>\n",
      "Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n",
      "</div>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "http_res = urlopen(\"http://pythonscraping.com/pages/page1.html\")\n",
    "bs = BeautifulSoup(http_res.read(), \"html.parser\")\n",
    "print(bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the help of `BeautifulSoup`, we have a better output. Besides, we can extract different parts in the html:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>A Useful Page</title>\n"
     ]
    }
   ],
   "source": [
    "print(bs.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>An Interesting Title</h1>\n"
     ]
    }
   ],
   "source": [
    "# same as bs.html.body.h1/bs.body.h1/bs.html.h1\n",
    "print(bs.h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div>\n",
      "Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n",
      "</div>\n"
     ]
    }
   ],
   "source": [
    "print(bs.div)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addtion to `html.parser`, `lxml` and `html5lib` are also userful. They are better at parsing \"messy\" or malformed HTML code. They can be used as:\n",
    "```\n",
    "bs = BeautifulSoup(http_res.read(), \"lxml\")\n",
    "bs = BeautifulSoup(http_res.read(), \"html5lib\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTTP requests don't always return a successful response; sometimes HTTP error will be returned. For example:\n",
    "- If the page is not found on the server, error code 404 (page not found) will be returned.\n",
    "- If the server is not found, error code 500 (internal server error) will be returned.\n",
    "\n",
    "To handle these cases, we need `HTTPError` and `URLError` in `urllib`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The server could not be found!\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "from urllib.error import URLError\n",
    "\n",
    "try:\n",
    "    http_res = urlopen('https://pythonscrapingthisurldoesnotexist.com')\n",
    "except HTTPError as e:\n",
    "    print(e)\n",
    "except URLError as e:\n",
    "    print(\"The server could not be found!\")\n",
    "else:\n",
    "    print(\"It Worked!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`find()` function in `BeautifulSoup` is handy and helpful. Here we give use an example of finding the first thing with tag `<h1>`. When we try to find something that doesn't exist, `find()` return `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "http_res = urlopen(\"http://pythonscraping.com/pages/page1.html\")\n",
    "bs = BeautifulSoup(http_res.read(), \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>An Interesting Title</h1>\n"
     ]
    }
   ],
   "source": [
    "print(bs.find(\"h1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(bs.find(\"notExistedTag\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding error handling to previous example, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>An Interesting Title</h1>\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def getTitle(url):\n",
    "    try:\n",
    "        http_res = urlopen(url)\n",
    "    except HTTPError as e:\n",
    "        return None\n",
    "    try:\n",
    "        bs = BeautifulSoup(http_res.read(), 'html.parser')\n",
    "        title = bs.body.h1\n",
    "    except AttributeError as e:\n",
    "        return None\n",
    "    return title\n",
    "\n",
    "title = getTitle('http://www.pythonscraping.com/pages/page1.html')\n",
    "if title == None:\n",
    "    print('Title could not be found')\n",
    "else:\n",
    "    print(title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
