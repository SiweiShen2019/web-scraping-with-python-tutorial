{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4 Web Crawling Models\n",
    "\n",
    "## Planning and Defining Objects\n",
    "\n",
    "This section in the textbook mainly talks about how to design objecgts to store scraped data. You can also think it as a pre-stage for storing data in the future. Please check the textbook for detailed explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Different Website Layouts\n",
    "\n",
    "Although it's ideal to have a general web crawler that works for all websites, human beings will pre-select a few websites needed to be scraped in reality. Here we give an example of using `Content` class to deal with both \"Brookings\" and \"New York Times\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wrap the process of using `BeautifulSoup` to scrape webpages into a utility function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPage(url):\n",
    "    \"\"\"\n",
    "    Utilty function used to get a BeautifulSoup object from a given URL\n",
    "    \"\"\"\n",
    "\n",
    "    session = requests.Session()\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36',\n",
    "               'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'}\n",
    "    try:\n",
    "        req = session.get(url, headers=headers)\n",
    "    except requests.exceptions.RequestException:\n",
    "        return None\n",
    "    bs = BeautifulSoup(req.text, 'html.parser')\n",
    "    return bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Content:\n",
    "    def __init__(self, url, title, body):\n",
    "        self.url = url\n",
    "        self.title = title\n",
    "        self.body = body\n",
    "\n",
    "\n",
    "def getPage(url):\n",
    "    req = requests.get(url)\n",
    "    return BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "\n",
    "def scrapeNYTimes(url):\n",
    "    bs = getPage(url)\n",
    "    title = bs.find('h1').text\n",
    "    lines = bs.select('div.StoryBodyCompanionColumn div p')\n",
    "    body = '\\n'.join([line.text for line in lines])\n",
    "    return Content(url, title, body)\n",
    "\n",
    "def scrapeBrookings(url):\n",
    "    bs = getPage(url)\n",
    "    title = bs.find('h1').text\n",
    "    body = bs.find('div', {'class', 'post-body'}).text\n",
    "    return Content(url, title, body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping \"Brookings\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Delivering inclusive urban access: 3 uncomfortable truths\n",
      "URL: https://www.brookings.edu/blog/future-development/2018/01/26/delivering-inclusive-urban-access-3-uncomfortable-truths/\n",
      "\n",
      "Length of BODY: 8234\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.brookings.edu/blog/future-development/2018/01/26/delivering-inclusive-urban-access-3-uncomfortable-truths/'\n",
    "content = scrapeBrookings(url)\n",
    "print('Title: {}'.format(content.title))\n",
    "print('URL: {}\\n'.format(content.url))\n",
    "# uncommnet to see the body\n",
    "# print(content.body)\n",
    "print(\"Length of BODY: {}\".format(len(content.body)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping \"New York Times\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Men Who Want to Live Forever\n",
      "URL: https://www.nytimes.com/2018/01/25/opinion/sunday/silicon-valley-immortality.html\n",
      "\n",
      "Length of BODY: 6515\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.nytimes.com/2018/01/25/opinion/sunday/silicon-valley-immortality.html'\n",
    "content = scrapeNYTimes(url)\n",
    "print('Title: {}'.format(content.title))\n",
    "print('URL: {}\\n'.format(content.url))\n",
    "# uncommnet to see the body\n",
    "# print(content.body)\n",
    "print(\"Length of BODY: {}\".format(len(content.body)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you find a pattern? We can wrap the printing process into a class function as well. Also, we can construct a `Website` class to store website information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Content:\n",
    "    \"\"\"\n",
    "    Common base class for all articles/pages\n",
    "    \"\"\"\n",
    "    def __init__(self, url, title, body):\n",
    "        self.url = url\n",
    "        self.title = title\n",
    "        self.body = body\n",
    "\n",
    "    def print(self):\n",
    "        \"\"\"\n",
    "        Flexible printing function controls output\n",
    "        \"\"\"\n",
    "        print('URL: {}'.format(self.url))\n",
    "        print('TITLE: {}'.format(self.title))\n",
    "        # uncommnet to see the body\n",
    "        # print('BODY:\\n{}'.format(self.body))\n",
    "        print(\"Length of BODY: {}\".format(len(self.body)))\n",
    "\n",
    "class Website:\n",
    "    \"\"\" \n",
    "    Contains information about website structure\n",
    "    \"\"\"\n",
    "    def __init__(self, name, url, titleTag, bodyTag):\n",
    "        self.name = name\n",
    "        self.url = url\n",
    "        self.titleTag = titleTag\n",
    "        self.bodyTag = bodyTag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a better `Crawler` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Crawler:\n",
    "\n",
    "    def getPage(self, url):\n",
    "        session = requests.Session()\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36',\n",
    "               'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'}\n",
    "        try:\n",
    "            req = session.get(url, headers=headers)\n",
    "        except requests.exceptions.RequestException:\n",
    "            return None\n",
    "        return BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "    def safeGet(self, pageObj, selector):\n",
    "        \"\"\"\n",
    "        Utilty function used to get a content string from a Beautiful Soup\n",
    "        object and a selector. Returns an empty string if no object\n",
    "        is found for the given selector\n",
    "        \"\"\"\n",
    "        selectedElems = pageObj.select(selector)\n",
    "        if selectedElems is not None and len(selectedElems) > 0:\n",
    "            return '\\n'.join([elem.get_text() for elem in selectedElems])\n",
    "        return ''\n",
    "\n",
    "    def parse(self, site, url):\n",
    "        \"\"\"\n",
    "        Extract content from a given page URL\n",
    "        \"\"\"\n",
    "        bs = self.getPage(url)\n",
    "        if bs is None:\n",
    "            print(\"Request exception caught in: {}\".format(url))\n",
    "        else:\n",
    "            title = self.safeGet(bs, site.titleTag)\n",
    "            body = self.safeGet(bs, site.bodyTag)\n",
    "            if title != '' and body != '':\n",
    "                content = Content(url, title, body)\n",
    "                content.print()\n",
    "            else:\n",
    "                print(\"Failed to load title and/or body: {}\".format(url))\n",
    "        print('-'*20)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on how we define `Crawler` class. We just need to list a series of websites and their tags respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: http://shop.oreilly.com/product/0636920028154.do\n",
      "TITLE: Learning Python, 5th Edition \n",
      "Length of BODY: 1306\n",
      "--------------------\n",
      "URL: http://www.reuters.com/article/us-usa-epa-pruitt-idUSKBN19W2D0\n",
      "TITLE: EPA chief wants scientists to debate climate on TV\n",
      "Length of BODY: 4863\n",
      "--------------------\n",
      "URL: https://www.brookings.edu/blog/techtank/2016/03/01/idea-to-retire-old-methods-of-policy-education/\n",
      "TITLE: Idea to Retire: Old methods of policy education\n",
      "Idea to Retire: Old methods of policy education\n",
      "Length of BODY: 9557\n",
      "--------------------\n",
      "URL: https://www.nytimes.com/2018/01/28/business/energy-environment/oil-boom.html\n",
      "TITLE: Oil Boom Gives the U.S. a New Edge in Energy and Diplomacy\n",
      "Length of BODY: 8498\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "crawler = Crawler()\n",
    "\n",
    "siteData = [\n",
    "    ['O\\'Reilly Media', 'http://oreilly.com', 'h1', 'section#product-description'],\n",
    "    ['Reuters', 'http://reuters.com', 'h1', 'div.StandardArticleBody_body'], # no \"_1gnLA\" is needed\n",
    "    ['Brookings', 'http://www.brookings.edu', 'h1', 'div.post-body'],\n",
    "    ['New York Times', 'http://nytimes.com', 'h1', 'div.StoryBodyCompanionColumn div p']\n",
    "]\n",
    "\n",
    "websites = []\n",
    "for row in siteData:\n",
    "    websites.append(Website(row[0], row[1], row[2], row[3]))\n",
    "\n",
    "crawler.parse(websites[0], 'http://shop.oreilly.com/product/0636920028154.do')\n",
    "crawler.parse(\n",
    "    websites[1], 'http://www.reuters.com/article/us-usa-epa-pruitt-idUSKBN19W2D0')\n",
    "crawler.parse(\n",
    "    websites[2],\n",
    "    'https://www.brookings.edu/blog/techtank/2016/03/01/idea-to-retire-old-methods-of-policy-education/')\n",
    "crawler.parse(\n",
    "    websites[3], \n",
    "    'https://www.nytimes.com/2018/01/28/business/energy-environment/oil-boom.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structuring Crawlers\n",
    "\n",
    "In this section, we focus on one of 3 different crawling structure introduced in the textbook: **Crawling Sites Through Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Content:\n",
    "    \"\"\"Common base class for all articles/pages\"\"\"\n",
    "\n",
    "    def __init__(self, topic, title, body, url):\n",
    "        self.topic = topic\n",
    "        self.title = title\n",
    "        self.body = body\n",
    "        self.url = url\n",
    "\n",
    "    def print(self):\n",
    "        \"\"\"\n",
    "        Flexible printing function controls output\n",
    "        \"\"\"\n",
    "        print('New article found for topic: {}'.format(self.topic))\n",
    "        print('URL: {}'.format(self.url))\n",
    "        print('TITLE: {}'.format(self.title))\n",
    "        # uncommnet to see the body\n",
    "        # print('BODY:\\n{}'.format(self.body))\n",
    "        print(\"Length of BODY: {}\".format(len(self.body)))\n",
    "\n",
    "\n",
    "class Website:\n",
    "    \"\"\"Contains information about website structure\"\"\"\n",
    "\n",
    "    def __init__(self, name, url, searchUrl, resultListing, resultUrl, absoluteUrl, titleTag, bodyTag):\n",
    "        self.name = name\n",
    "        self.url = url\n",
    "        self.searchUrl = searchUrl\n",
    "        self.resultListing = resultListing\n",
    "        self.resultUrl = resultUrl\n",
    "        self.absoluteUrl = absoluteUrl\n",
    "        self.titleTag = titleTag\n",
    "        self.bodyTag = bodyTag\n",
    "\n",
    "        \n",
    "class Crawler:\n",
    "\n",
    "    def getPage(self, url):\n",
    "        session = requests.Session()\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36',\n",
    "               'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'}\n",
    "        try:\n",
    "            req = session.get(url, headers=headers)\n",
    "        except requests.exceptions.RequestException:\n",
    "            return None\n",
    "        return BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "    def safeGet(self, pageObj, selector):\n",
    "        childObj = pageObj.select(selector)\n",
    "        if childObj is not None and len(childObj) > 0:\n",
    "            return childObj[0].get_text()\n",
    "        return ''\n",
    "\n",
    "    def search(self, topic, site):\n",
    "        \"\"\"\n",
    "        Searches a given website for a given topic and records all pages found\n",
    "        \"\"\"\n",
    "        bs = self.getPage(site.searchUrl + topic)\n",
    "        searchResults = bs.select(site.resultListing)\n",
    "        for result in searchResults[:3]: # limit to first 3 results\n",
    "            url = result.select(site.resultUrl)[0].attrs['href']\n",
    "            # Check to see whether it's a relative or an absolute URL\n",
    "            if(site.absoluteUrl):\n",
    "                bs = self.getPage(url)\n",
    "            else:\n",
    "                bs = self.getPage(site.url + url)\n",
    "            if bs is None:\n",
    "                print('Something was wrong with that page or URL. Skipping!')\n",
    "                print('-'*20)\n",
    "                return\n",
    "            title = self.safeGet(bs, site.titleTag)\n",
    "            body = self.safeGet(bs, site.bodyTag)\n",
    "            if title != '' and body != '':\n",
    "                content = Content(topic, title, body, url)\n",
    "                content.print()\n",
    "            else:\n",
    "                print(\"Failed to load title and/or body: {}\".format(url))\n",
    "            print('-'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "GETTING INFO ABOUT: python\n",
      "****************************************\n",
      "New article found for topic: python\n",
      "URL: http://shop.oreilly.com/product/0636920028154.do\n",
      "TITLE: Learning Python, 5th Edition \n",
      "Length of BODY: 1306\n",
      "--------------------\n",
      "Failed to load title and/or body: https://www.oreilly.com/programming/free/python-data-for-developers.csp\n",
      "--------------------\n",
      "Failed to load title and/or body: https://www.oreilly.com/programming/free/python-for-scientists.csp\n",
      "--------------------\n",
      "New article found for topic: python\n",
      "URL: /article/idUSKBN1OD2CM\n",
      "TITLE: UK woman illegally imported python-skin products\n",
      "Length of BODY: 1262\n",
      "--------------------\n",
      "New article found for topic: python\n",
      "URL: /article/idUSKCN11S04G\n",
      "TITLE: Python in India demonstrates huge appetite\n",
      "Length of BODY: 1008\n",
      "--------------------\n",
      "New article found for topic: python\n",
      "URL: /article/idUSKBN0L31PS20150130\n",
      "TITLE: Zimbabwean jailed for nine years for eating python meat\n",
      "Length of BODY: 617\n",
      "--------------------\n",
      "New article found for topic: python\n",
      "URL: https://www.brookings.edu/blog/up-front/2015/12/21/the-hutchins-center-explains-budgeting-for-aging-america/\n",
      "TITLE: The Hutchins Center Explains: Budgeting for aging America\n",
      "Length of BODY: 3918\n",
      "--------------------\n",
      "New article found for topic: python\n",
      "URL: https://www.brookings.edu/opinions/inside-the-pentagons-secret-afghan-spy-machine/\n",
      "TITLE: Inside the Pentagon’s Secret Afghan Spy Machine\n",
      "Length of BODY: 306\n",
      "--------------------\n",
      "New article found for topic: python\n",
      "URL: https://www.brookings.edu/blog/techtank/2017/11/16/leveraging-the-disruptive-power-of-artificial-intelligence-for-fairer-opportunities/\n",
      "TITLE: Leveraging the disruptive power of artificial intelligence for fairer opportunities\n",
      "Length of BODY: 6875\n",
      "--------------------\n",
      "****************************************\n",
      "GETTING INFO ABOUT: data science\n",
      "****************************************\n",
      "Failed to load title and/or body: https://www.oreilly.com/data/free/building-data-science-teams.csp\n",
      "--------------------\n",
      "Failed to load title and/or body: https://www.oreilly.com/data/free/2014-data-science-salary-survey.csp\n",
      "--------------------\n",
      "New article found for topic: data science\n",
      "URL: http://shop.oreilly.com/product/0636920090229.do\n",
      "TITLE: Data Science Fundamentals for Marketing and Business Professionals\n",
      "Length of BODY: 1388\n",
      "--------------------\n",
      "New article found for topic: data science\n",
      "URL: /article/idUSL5N21Z1AH\n",
      "TITLE: BRIEF-AB Science Presents New Preclinical Data For Masitinib In ALS\n",
      "Length of BODY: 379\n",
      "--------------------\n",
      "New article found for topic: data science\n",
      "URL: /article/idUSKCN1LZ1EX\n",
      "TITLE: Greenland and the hunt for better climate science\n",
      "Length of BODY: 8554\n",
      "--------------------\n",
      "New article found for topic: data science\n",
      "URL: /article/idUSKCN1ME1CT\n",
      "TITLE: Blackstone buys Clarus to bulk up in life sciences\n",
      "Length of BODY: 2829\n",
      "--------------------\n",
      "New article found for topic: data science\n",
      "URL: https://www.brookings.edu/blog/brown-center-chalkboard/2016/03/10/big-data-meet-behavioral-science/\n",
      "TITLE: Big data, meet behavioral science\n",
      "Length of BODY: 7139\n",
      "--------------------\n",
      "New article found for topic: data science\n",
      "URL: https://www.brookings.edu/blog/brookings-now/2018/02/09/charts-of-the-week-advancing-women-and-girls-in-science/\n",
      "TITLE: Charts of the week: Advancing women and girls in science\n",
      "Length of BODY: 3943\n",
      "--------------------\n",
      "New article found for topic: data science\n",
      "URL: https://www.brookings.edu/blog/techtank/2018/05/17/artificial-intelligence-and-data-analytics-in-india/\n",
      "TITLE: Artificial intelligence and data analytics in India\n",
      "Length of BODY: 9851\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "crawler = Crawler()\n",
    "\n",
    "siteData = [\n",
    "    ['O\\'Reilly Media', 'http://oreilly.com', 'https://ssearch.oreilly.com/?q=',\n",
    "        'article.product-result', 'p.title a', True, 'h1', 'section#product-description'], # work only for non-free book\n",
    "    ['Reuters', 'http://reuters.com', 'http://www.reuters.com/search/news?blob=', 'div.search-result-content',\n",
    "        'h3.search-result-title a', False, 'h1', 'div.StandardArticleBody_body'],\n",
    "    ['Brookings', 'http://www.brookings.edu', 'https://www.brookings.edu/search/?s=',\n",
    "        'div.list-content article', 'h4.title a', True, 'h1', 'div.post-body']\n",
    "]\n",
    "sites = []\n",
    "for row in siteData:\n",
    "    sites.append(Website(row[0], row[1], row[2],\n",
    "                         row[3], row[4], row[5], row[6], row[7]))\n",
    "\n",
    "topics = ['python', 'data science']\n",
    "for topic in topics:\n",
    "    print('*'*40)\n",
    "    print('GETTING INFO ABOUT: ' + topic)\n",
    "    print('*'*40)\n",
    "    for targetSite in sites:\n",
    "        crawler.search(topic, targetSite)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
